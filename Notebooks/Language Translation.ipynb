{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Model To Translate English To Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU ,Embedding, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, LSTM, Dropout\n",
    "from keras.optimizers import Adam \n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  English words/sentences French words/sentences\n",
      "0                     Hi.                 Salut!\n",
      "1                    Run!                Cours !\n",
      "2                    Run!               Courez !\n",
      "3                    Who?                  Qui ?\n",
      "4                    Wow!             Ça alors !\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"D:\\DML\\English-French\\eng_-french.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English words/sentences    0\n",
       "French words/sentences     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = data[\"English words/sentences\"]\n",
    "french = data[\"French words/sentences\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words Counter in English and French "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnglishWordsCounter = collections.Counter([word for sentence in english for word in sentence.split()])\n",
    "FrenchWordsCounter = collections.Counter([word for sentence in french for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Words: 1082098\n",
      "English Unique Words: 27393\n",
      "20 Most Commons Words in English data: \n",
      "\"I\" \"to\" \"you\" \"the\" \"a\" \"is\" \"Tom\" \"of\" \"in\" \"have\" \"was\" \"that\" \"He\" \"I'm\" \"for\" \"don't\" \"do\" \"You\" \"your\" \"be\"\n",
      "\n",
      "French Words: 1177832\n",
      "French Unique Words: 44918\n",
      "20 Most Commons Words in French data: \n",
      "\"de\" \"Je\" \"?\" \"pas\" \"que\" \"à\" \"ne\" \"la\" \"le\" \"Il\" \"Tom\" \"est\" \"vous\" \"un\" \"a\" \"ce\" \"en\" \"une\" \"me\" \"je\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('English Words: {}'.format(len([word for sentence in english for word in sentence.split()])))\n",
    "print('English Unique Words: {}'.format(len(EnglishWordsCounter)))\n",
    "print('20 Most Commons Words in English data: ')\n",
    "print('\"' + '\" \"'.join(list(zip(*EnglishWordsCounter.most_common(20)))[0]) + '\"')\n",
    "print()\n",
    "\n",
    "print('French Words: {}'.format(len([word for sentence in french for word in sentence.split()])))\n",
    "print('French Unique Words: {}'.format(len(FrenchWordsCounter)))\n",
    "print('20 Most Commons Words in French data: ')\n",
    "print('\"' + '\" \"'.join(list(zip(*FrenchWordsCounter.most_common(20)))[0]) + '\"')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization divides the original text into tokens, which are words and sentences.\n",
    "def TokenFunction(t):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(t)\n",
    "    return tokenizer.texts_to_sequences(t), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pad_sequences to ensure that all sequences have the same the length\n",
    "def padSequences(t, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in t])\n",
    "    return pad_sequences(t, maxlen = 55, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessFunction(x,y):\n",
    "    preprocessX, tokenX = TokenFunction(x)\n",
    "    preprocessY, tokenY = TokenFunction(y)\n",
    "    \n",
    "    preprocessX = padSequences(preprocessX)\n",
    "    preprocessY = pad_sequences(preprocessY)\n",
    "    preprocessY = preprocessY.reshape(*preprocessY.shape, 1)\n",
    "    \n",
    "    return preprocessX,preprocessY, tokenX,tokenY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessEnglishSentences, preprocessFrenchSentences, englishTokenizer, frenchTokenizer = preprocessFunction(english,french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum English Sentence Length:  55\n",
      "Maximum French Sentence Length:  55\n",
      "English Vocabulary Size:  14531\n",
      "French Vocabulary Size:  30660\n"
     ]
    }
   ],
   "source": [
    "maxEnglishSequenceLength = preprocessEnglishSentences.shape[1]\n",
    "maxFrenchSequenceLength = preprocessFrenchSentences.shape[1]\n",
    "\n",
    "englishVocabSize = len(englishTokenizer.word_index)\n",
    "frenchVocabSize = len(frenchTokenizer.word_index)\n",
    "\n",
    "print(\"Maximum English Sentence Length: \",maxEnglishSequenceLength)\n",
    "print(\"Maximum French Sentence Length: \",maxFrenchSequenceLength)\n",
    "\n",
    "print(\"English Vocabulary Size: \", englishVocabSize)\n",
    "print(\"French Vocabulary Size: \", frenchVocabSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Final Prediction Into Text Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textConverter(index, tokenizer):\n",
    "    indexTOWords = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    indexTOWords[0] = 'CONVERTER'\n",
    "    return ' '.join([indexTOWords[prediction] for prediction in np.argmax(index, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Bidirectional RNN Model With Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BidirectionalModel(inputShape, outputSequenceLength, englishVocabSize, frenchVocabSize):\n",
    "    learningRate = 0.003\n",
    "    \n",
    "    #Layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(frenchVocabSize,256,input_length=inputShape[1], input_shape=inputShape[1:]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(englishVocabSize, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy, optimizer= Adam(learningRate), metrics=['Accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175621, 55)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessEnglishSentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 55, 256)           7849216   \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 55, 512)          789504    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 55, 1024)         525312    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 55, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 55, 14532)        14895300  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,059,332\n",
      "Trainable params: 24,059,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Reshape the input\n",
    "inputReshape = padSequences(preprocessFrenchSentences, preprocessFrenchSentences.shape[1])\n",
    "inputReshape = inputReshape.reshape((-1, preprocessFrenchSentences.shape[-2]))\n",
    "\n",
    "#Train  Model\n",
    "model = BidirectionalModel(inputReshape.shape, preprocessEnglishSentences.shape[1],len(englishTokenizer.word_index)+1,len(frenchTokenizer.word_index)+1)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/CPU:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     model\u001b[39m.\u001b[39mfit(inputReshape, preprocessEnglishSentences, batch_size\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, validation_split\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    model.fit(inputReshape, preprocessEnglishSentences, batch_size=0, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Converter(text, tokenizer):\n",
    "     index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "     index_to_words[0] = '<CONVERTER>'\n",
    "\n",
    "     return ' '.join([index_to_words[prediction] for prediction in np.argmax(text, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18e3fd758e2d79b5ad5bb08623e97769ba332cf1a2179fda988744caa974d24a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
